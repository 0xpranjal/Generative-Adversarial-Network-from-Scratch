{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation, Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code blocks mentioned below step 2 and step 3 define two different neural networks. The only major difference between generator and discriminator network are inputs and outputs\n",
    "\n",
    "> The Generator networks takes random noise as input and tries to recreate the images from the training set.\n",
    "\n",
    "> The discriminator is binary classifier tries to distinguish the images generated by the generator network from the actual train images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    #initializing the neural network\n",
    "    generator= Sequential()\n",
    "    #adding an input layer to the network\n",
    "    generator.add(Dense(units=256, input_dim=100))\n",
    "    #activating the layer with LeakyReLU activation function\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    #applying batch Normalization\n",
    "    generator.add(Dense(units=512))\n",
    "    #adding the third layer\n",
    "    generator.add(Dense(units=1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    #the output layer with 784(28x28) nodes\n",
    "    generator.add(Dense(units=784, activation='tanh'))\n",
    "    #compiling the generator network with loss and optimizer functions\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam(lr=0.0002, beta_1=0.5))\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    #Initializing a neural network\n",
    "    discriminator=Sequential()\n",
    "    \n",
    "    #Adding an Input layer to the network\n",
    "    discriminator.add(Dense(units=1024, input_dim=784))\n",
    "    \n",
    "    #Activating the layer with LeakyReLU activation function\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    #Adding a dropout layer to reduce overfitting\n",
    "    discriminator.add(Dropout(0.2))\n",
    "       \n",
    "    #Adding a second layer\n",
    "    discriminator.add(Dense(units=512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "       \n",
    "    #Adding a third layer\n",
    "    discriminator.add(Dense(units=256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    #Adding a forth layer\n",
    "    discriminator.add(Dense(units=128))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    #Adding the output layer with sigmoid activation\n",
    "    discriminator.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    #Compiling the Discriminator Network with loss and optimizer functions\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer = keras.optimizers.adam(lr=0.0002, beta_1=0.5))\n",
    "    \n",
    "    return discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a GAN Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a GAN by stacking the generator and discriminator networks\n",
    "\n",
    "The trainable parameter of the discriminator network when set to false freezes the weights in the discriminator network while the generator network is trained. This prevents the discriminator network from being updated while the generator generates new image from noise.\n",
    "\n",
    "The input shape to the GAN network is the shape of the noise. The noise is fed to the generator and its output is fed to the discriminator which classifies the image as original or generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking The Generator And Discriminator Networks To Form A GAN\n",
    "\n",
    "def gan_net(generator, discriminator):\n",
    "  \n",
    "    #Setting the trainable parameter of discriminator to False\n",
    "    discriminator.trainable=False\n",
    "    \n",
    "    #Instantiates a Keras tensor of shape 100 (Noise shape)\n",
    "    inp = Input(shape=(100,))\n",
    "    \n",
    "    #Feeds the input noise to the generator and stores the output in X\n",
    "    X = generator(inp)\n",
    "    \n",
    "    #Feeds the output from generator(X) to the discriminator and stores the result in out\n",
    "    out= discriminator(X)\n",
    "    \n",
    "    #Creates a model include all layers required in the computation of out given inp.\n",
    "    gan= Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    #Compiling the GAN Network\n",
    "    gan.compile(loss='binary_crossentropy', optimizer = 'adam')\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to plot the images\n",
    "def plot_images(epoch, generator, dim = (10,10), figsize=(15,15)):\n",
    "    #Generate a normally distributed noise of shape(100x100)\n",
    "    noise= np.random.normal(loc=0, scale=1, size=[100, 100]) \n",
    "    #Generate an image for the input noise\n",
    "    generated_images = generator.predict(noise)\n",
    "    #Reshape the generated image \n",
    "    generated_images = generated_images.reshape(100,28,28)\n",
    "    \n",
    "    #Plot the image \n",
    "    plt.figure(figsize=figsize)\n",
    "   \n",
    "    #Plot for each pixel\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i],cmap='gray', interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traing method with training set, default epoch and default batch_size as arguments.\n",
    "\n",
    "def train(X_train, epochs=5, batch_size=128):\n",
    "    \n",
    "    \n",
    "    #Initializing the GAN \n",
    "    generator= build_generator()\n",
    "    discriminator= build_discriminator()\n",
    "    gan = gan_net(generator,discriminator)\n",
    "    \n",
    "    \n",
    "    # Training the model for specified epochs\n",
    "    \n",
    "    for epoch in range(1,epochs+1 ):\n",
    "        print(\"###### @ Epoch \", epoch)\n",
    "        \n",
    "        #tqdm module helps to generate a status bar for training \n",
    "        for _ in tqdm(range(batch_size)):\n",
    "          \n",
    "            #Random noise with size batch_sizex100\n",
    "            noise= np.random.normal(0,1, [batch_size, 100])\n",
    "            \n",
    "            #Generating images from noise\n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            #taking random images from the training set \n",
    "            image_batch =X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n",
    "            \n",
    "            #Creating a new training set with real and fake images \n",
    "            X= np.concatenate([image_batch, generated_images])\n",
    "            \n",
    "            # Labels for generated and real data\n",
    "            y_dis=np.zeros(2*batch_size)  \n",
    "            y_dis[:batch_size]=1.0 # label for real images\n",
    "            \n",
    "            #Training the discriminator with real and generated images\n",
    "            discriminator.trainable=True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            \n",
    "            #Labelling the generated images a sreal images(1) to trick the discriminator\n",
    "            noise= np.random.normal(0,1, [batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            \n",
    "            #Freezing the weights of the discriminator while training generator\n",
    "            discriminator.trainable=False\n",
    "            \n",
    "            #Training the gan network\n",
    "            gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        #Plotting the images for every 10 epochs\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "          plot_images(epoch,generator,dim = (10,10) , figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpacking the training data from mnist data dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "#Converting to float type and normalizing the data\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "\n",
    "# convert shape of X_train from (60000, 28, 28) to (60000, 784) - 784 columns per row\n",
    "X_train = X_train.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:11<00:00, 11.16it/s]\n",
      "  1%|          | 1/128 [00:00<00:13,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:10<00:00, 12.17it/s]\n",
      "  2%|▏         | 2/128 [00:00<00:10, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:14<00:00,  8.84it/s]\n",
      "  2%|▏         | 2/128 [00:00<00:10, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:13<00:00,  9.53it/s]\n",
      "  2%|▏         | 2/128 [00:00<00:11, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:12<00:00, 10.30it/s]\n",
      "  1%|          | 1/128 [00:00<00:17,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:11<00:00, 11.04it/s]\n",
      "  2%|▏         | 2/128 [00:00<00:09, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:13<00:00,  9.38it/s]\n",
      "  2%|▏         | 2/128 [00:00<00:11, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### @ Epoch  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 110/128 [00:09<00:01, 11.67it/s]"
     ]
    }
   ],
   "source": [
    "train(X_train,epochs = 20, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
